{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23063, 238)\n",
      "(23063,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def preprocess_training():\n",
    "    # Most of this is a task in preprocessing appropriately\n",
    "\n",
    "    train_data = pd.read_csv(\"./project_data_files/book_rating_train.csv\")\n",
    "\n",
    "    # These are strings we need to handle with provided files\n",
    "    train_name = pd.read_csv(\"./project_data_files/book_text_features_doc2vec/train_name_doc2vec100.csv\", index_col = False, delimiter = ',', header=None)\n",
    "    train_authors = pd.read_csv(\"./project_data_files/book_text_features_doc2vec/train_authors_doc2vec20.csv\", index_col = False, delimiter = ',', header=None)\n",
    "    train_desc = pd.read_csv(\"./project_data_files/book_text_features_doc2vec/train_desc_doc2vec100.csv\", index_col = False, delimiter = ',', header=None)\n",
    "\n",
    "\n",
    "    # Give these all column names which aren't ints\n",
    "    train_name = train_name.set_axis([f\"name-{x}\" for x in train_name.columns], axis=1)\n",
    "    train_authors = train_authors.set_axis([f\"author-{x}\" for x in train_authors.columns], axis=1)\n",
    "    train_desc = train_desc.set_axis([f\"desc-{x}\" for x in train_desc.columns], axis=1)\n",
    "\n",
    "    # Transform categorical values into useful vectors\n",
    "    train_lang = pd.get_dummies(train_data[['Language']])\n",
    "    train_lang = train_lang.drop(['Language_ara', 'Language_frs', 'Language_heb', 'Language_zho', 'Language_lat'], axis=1)\n",
    "\n",
    "    cv = CountVectorizer()\n",
    "    train_publisher = pd.DataFrame(cv.fit_transform(train_data[['Publisher']]).todense())\n",
    "    train_publisher = train_publisher.set_axis([f\"publisher-char-{x}\" for x in train_publisher.columns], axis=1)\n",
    "\n",
    "    # Merge all the pieces together\n",
    "    combine = pd.concat([train_name, train_desc, train_authors, train_publisher, train_lang], axis=1)\n",
    "\n",
    "    # Add the remaining attributes\n",
    "    X_train = pd.concat([combine, train_data[['PublishYear', 'PublishMonth', 'PublishDay', 'pagesNumber']]], axis=1).fillna(0)\n",
    "    y_train = train_data['rating_label']\n",
    "\n",
    "    return X_train, y_train\n",
    "\n",
    "# Get the preprocessed data for doc2vec\n",
    "X1, y1 = preprocess_training()\n",
    "\n",
    "print(X1.shape)\n",
    "print(y1.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5766, 38)\n",
      "['name-10', 'desc-2', 'name-33', 'desc-28', 'desc-90', 'desc-3', 'name-34', 'name-70', 'desc-4', 'desc-70', 'desc-87', 'name-24', 'desc-58', 'desc-36', 'desc-71', 'name-72', 'name-11', 'name-20', 'name-67', 'name-31', 'desc-93', 'desc-55', 'desc-56', 'name-6', 'desc-14', 'name-69', 'desc-65', 'name-15', 'name-26', 'desc-21', 'name-25', 'desc-94', 'desc-37', 'desc-91', 'name-16', 'name-48', 'name-5', 'desc-77', 'desc-48', 'name-13', 'desc-12', 'name-28', 'desc-96', 'name-21', 'name-47', 'desc-11', 'desc-53', 'desc-99', 'name-44', 'name-74', 'name-85', 'desc-64', 'desc-95', 'desc-63', 'desc-86', 'desc-16', 'name-46', 'desc-20', 'desc-31', 'name-8', 'desc-69', 'desc-5', 'name-98', 'name-18', 'desc-6', 'desc-46', 'desc-49', 'desc-60', 'desc-80', 'name-56', 'desc-41', 'desc-26', 'name-36', 'name-45', 'name-87', 'desc-7', 'desc-44', 'desc-30', 'desc-83', 'desc-84', 'name-89', 'desc-34', 'name-43', 'name-52', 'name-68', 'desc-97', 'desc-18', 'name-83', 'desc-68', 'desc-9', 'name-9', 'name-99', 'name-55', 'desc-43', 'name-12', 'desc-50', 'desc-38', 'desc-76', 'desc-61', 'desc-39', 'desc-8', 'name-58', 'name-80', 'name-57', 'name-91', 'desc-45', 'name-92', 'name-82', 'name-27', 'desc-33', 'name-86', 'name-64', 'desc-75', 'name-7', 'desc-13', 'name-2', 'desc-54', 'desc-35', 'desc-25', 'name-93', 'desc-47', 'name-32', 'name-51', 'desc-17', 'desc-85', 'desc-32', 'desc-92', 'name-54', 'desc-0', 'desc-72', 'name-4', 'name-29', 'name-53', 'desc-74', 'name-81', 'name-0', 'name-40', 'name-90', 'desc-19', 'name-77', 'name-37', 'desc-98', 'desc-42', 'desc-57', 'name-35', 'name-14', 'name-62', 'name-3', 'desc-22', 'desc-51', 'desc-78', 'name-42', 'name-41', 'desc-88', 'name-97', 'desc-66', 'name-75', 'desc-24', 'desc-62', 'name-94', 'name-1', 'name-22', 'name-78', 'name-17', 'name-88', 'desc-81', 'name-96', 'name-79', 'name-73', 'name-61', 'desc-1', 'desc-40', 'name-23', 'name-60', 'name-30', 'desc-15', 'name-59', 'name-50', 'name-63', 'name-49', 'desc-27', 'name-95', 'desc-89', 'desc-73', 'desc-79', 'name-39', 'name-65', 'name-66', 'name-84', 'name-19', 'desc-23', 'desc-82', 'name-71', 'desc-52', 'desc-59', 'desc-67', 'desc-29', 'desc-10', 'name-76', 'name-38']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_testing():\n",
    "    # Most of this is a task in preprocessing appropriately\n",
    "\n",
    "    train_data = pd.read_csv(\"./project_data_files/book_rating_test.csv\")\n",
    "\n",
    "    # These are strings we need to handle with provided files\n",
    "    train_name = pd.read_csv(\"./project_data_files/book_text_features_doc2vec/test_name_doc2vec100.csv\", index_col = False, delimiter = ',', header=None)\n",
    "    train_authors = pd.read_csv(\"./project_data_files/book_text_features_doc2vec/test_authors_doc2vec20.csv\", index_col = False, delimiter = ',', header=None)\n",
    "    train_desc = pd.read_csv(\"./project_data_files/book_text_features_doc2vec/test_desc_doc2vec100.csv\", index_col = False, delimiter = ',', header=None)\n",
    "\n",
    "\n",
    "    # Give these all column names which aren't ints\n",
    "    train_name = train_name.set_axis([f\"name-{x}\" for x in train_name.columns], axis=1)\n",
    "    train_authors = train_authors.set_axis([f\"author-{x}\" for x in train_authors.columns], axis=1)\n",
    "    train_desc = train_desc.set_axis([f\"desc-{x}\" for x in train_desc.columns], axis=1)\n",
    "\n",
    "    # Transform categorical values into useful vectors\n",
    "    train_lang = pd.get_dummies(train_data[['Language']])\n",
    "    # Remove languages not found in the training data - they tell us nothing\n",
    "    train_lang = train_lang.drop(['Language_hun', 'Language_urd', 'Language_tha', 'Language_glg'], axis=1)\n",
    "\n",
    "    cv = CountVectorizer()\n",
    "    train_publisher = pd.DataFrame(cv.fit_transform(train_data[['Publisher']]).todense())\n",
    "    train_publisher = train_publisher.set_axis([f\"publisher-char-{x}\" for x in train_publisher.columns], axis=1)\n",
    "\n",
    "    # Merge all the pieces together\n",
    "    combine = pd.concat([train_authors, train_publisher, train_lang], axis=1)\n",
    "\n",
    "    # Add the remaining attributes\n",
    "    X_train = pd.concat([combine, train_data[['PublishYear', 'PublishMonth', 'PublishDay', 'pagesNumber']]], axis=1).fillna(0)\n",
    "\n",
    "    return X_train\n",
    "\n",
    "X_test = preprocess_testing()\n",
    "print(X_test.shape)\n",
    "\n",
    "print(list(set(X1.columns) - set(X_test.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running models on doc2vec set\n",
      "Running model Random Forest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 93\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mGenerated predictions for\u001b[39m\u001b[39m\"\u001b[39m, title)\n\u001b[1;32m     92\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRunning models on doc2vec set\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m run_models(X1, y1, X_test)\n",
      "Cell \u001b[0;32mIn[106], line 83\u001b[0m, in \u001b[0;36mrun_models\u001b[0;34m(X_train, y_train, X_test)\u001b[0m\n\u001b[1;32m     75\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     77\u001b[0m \u001b[39m# X1, X2, y1, y2 = train_test_split(X_train, y_train, test_size=0.2)\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[39m# model.fit(X1, y1)\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39m# y_pred = model.predict(X2)\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[39m# print(classification_report(y2, y_pred))\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[39m# print(\"Accuracy is\", accuracy_score(y2, y_pred))\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m acc \u001b[39m=\u001b[39m cross_validate(model, X_train, y_train)\n\u001b[1;32m     84\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAcc was\u001b[39m\u001b[39m\"\u001b[39m, np\u001b[39m.\u001b[39mmean(acc[\u001b[39m'\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m'\u001b[39m]), \u001b[39m\"\u001b[39m\u001b[39mfor model\u001b[39m\u001b[39m\"\u001b[39m, title)\n\u001b[1;32m     86\u001b[0m \u001b[39m#format_test_pred(model, X_train, y_train, X_test, title)\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    268\u001b[0m         clone(estimator),\n\u001b[1;32m    269\u001b[0m         X,\n\u001b[1;32m    270\u001b[0m         y,\n\u001b[1;32m    271\u001b[0m         scorers,\n\u001b[1;32m    272\u001b[0m         train,\n\u001b[1;32m    273\u001b[0m         test,\n\u001b[1;32m    274\u001b[0m         verbose,\n\u001b[1;32m    275\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    276\u001b[0m         fit_params,\n\u001b[1;32m    277\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[1;32m    278\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    279\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[1;32m    280\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m    281\u001b[0m     )\n\u001b[1;32m    282\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m cv\u001b[39m.\u001b[39;49msplit(X, y, groups)\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;49;00m func, args, kwargs \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mitems]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39;49mfit(X_train, y_train, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/ensemble/_bagging.py:337\u001b[0m, in \u001b[0;36mBaseBagging.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[39m# Convert data (X is required to be 2d and indexable)\u001b[39;00m\n\u001b[1;32m    329\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(\n\u001b[1;32m    330\u001b[0m     X,\n\u001b[1;32m    331\u001b[0m     y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    335\u001b[0m     multi_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    336\u001b[0m )\n\u001b[0;32m--> 337\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_samples, sample_weight\u001b[39m=\u001b[39;49msample_weight)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/ensemble/_bagging.py:472\u001b[0m, in \u001b[0;36mBaseBagging._fit\u001b[0;34m(self, X, y, max_samples, max_depth, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    469\u001b[0m seeds \u001b[39m=\u001b[39m random_state\u001b[39m.\u001b[39mrandint(MAX_INT, size\u001b[39m=\u001b[39mn_more_estimators)\n\u001b[1;32m    470\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_seeds \u001b[39m=\u001b[39m seeds\n\u001b[0;32m--> 472\u001b[0m all_results \u001b[39m=\u001b[39m Parallel(\n\u001b[1;32m    473\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs, verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parallel_args()\n\u001b[1;32m    474\u001b[0m )(\n\u001b[1;32m    475\u001b[0m     delayed(_parallel_build_estimators)(\n\u001b[1;32m    476\u001b[0m         n_estimators[i],\n\u001b[1;32m    477\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    478\u001b[0m         X,\n\u001b[1;32m    479\u001b[0m         y,\n\u001b[1;32m    480\u001b[0m         sample_weight,\n\u001b[1;32m    481\u001b[0m         seeds[starts[i] : starts[i \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m]],\n\u001b[1;32m    482\u001b[0m         total_n_estimators,\n\u001b[1;32m    483\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    484\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[1;32m    485\u001b[0m     )\n\u001b[1;32m    486\u001b[0m     \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(n_jobs)\n\u001b[1;32m    487\u001b[0m )\n\u001b[1;32m    489\u001b[0m \u001b[39m# Reduce\u001b[39;00m\n\u001b[1;32m    490\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_ \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[1;32m    491\u001b[0m     itertools\u001b[39m.\u001b[39mchain\u001b[39m.\u001b[39mfrom_iterable(t[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m all_results)\n\u001b[1;32m    492\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1077\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 1085\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1088\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;49;00m func, args, kwargs \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mitems]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/ensemble/_bagging.py:141\u001b[0m, in \u001b[0;36m_parallel_build_estimators\u001b[0;34m(n_estimators, ensemble, X, y, sample_weight, seeds, total_n_estimators, verbose, check_input)\u001b[0m\n\u001b[1;32m    138\u001b[0m         curr_sample_weight[not_indices_mask] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    140\u001b[0m     X_ \u001b[39m=\u001b[39m X[:, features] \u001b[39mif\u001b[39;00m requires_feature_indexing \u001b[39melse\u001b[39;00m X\n\u001b[0;32m--> 141\u001b[0m     estimator_fit(X_, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight)\n\u001b[1;32m    142\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     X_ \u001b[39m=\u001b[39m X[indices][:, features] \u001b[39mif\u001b[39;00m requires_feature_indexing \u001b[39melse\u001b[39;00m X[indices]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/tree/_classes.py:889\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    860\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \n\u001b[1;32m    862\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    890\u001b[0m         X,\n\u001b[1;32m    891\u001b[0m         y,\n\u001b[1;32m    892\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    893\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    895\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/tree/_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    370\u001b[0m         splitter,\n\u001b[1;32m    371\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    377\u001b[0m     )\n\u001b[0;32m--> 379\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight)\n\u001b[1;32m    381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[1;32m    382\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "def format_test_pred(model, X_train, y_train, X_test, title):\n",
    "        model.fit(X_train, y_train)\n",
    "        print(\"Fitted model\")\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(\"Made predictions\")\n",
    "        print(\"Max prediction\", max(y_pred))\n",
    "        y_pred = pd.Series(y_pred, name=\"rating_label\", index=range(1,len(y_pred)+1))\n",
    "        y_pred.index.name = \"id\"\n",
    "        y_pred.to_csv(f\"./model_predictions/{title}.csv\")\n",
    "\n",
    "def feature_filter(X_train, y_train, X_test):\n",
    "        selector = SelectKBest(k=20, score_func=mutual_info_classif)\n",
    "        print(\"Filtering features\")\n",
    "        X_train = pd.DataFrame(selector.fit_transform(X_train, y_train))\n",
    "        for feat_num in selector.get_support(indices=True):\n",
    "                print(selector.get_feature_names_out()[feat_num])\n",
    "\n",
    "        X_test = pd.DataFrame(selector.transform(X_test))\n",
    "        print(\"Chose best features as\", X_train.head())\n",
    "        return X_train, y_train, X_test\n",
    "\n",
    "\n",
    "def run_models(X_train, y_train, X_test):\n",
    "    models = [\n",
    "          AdaBoostClassifier(),\n",
    "          GaussianNB(),\n",
    "          MLPClassifier(learning_rate=\"adaptive\", activation=\"tanh\", hidden_layer_sizes=[100, 10]),\n",
    "          LinearSVC(),\n",
    "          LinearRegression(),\n",
    "          SVC(kernel='poly', degree=3),\n",
    "          DecisionTreeClassifier(criterion=\"log_loss\"),\n",
    "          BaggingClassifier(base_estimator=DecisionTreeClassifier(criterion=\"log_loss\"),n_estimators=10,\\\n",
    "                              max_samples=0.8, max_features=0.8),\n",
    "          KNeighborsClassifier(n_neighbors=2),\n",
    "          KNeighborsClassifier(n_neighbors=7),\n",
    "          LogisticRegression(max_iter = 500),\n",
    "          DummyClassifier(strategy=\"most_frequent\"),\n",
    "          ]\n",
    "    titles = [\n",
    "        'ADA Boosting',\n",
    "        'GNB',\n",
    "            'MLP',\n",
    "            'LinearSVC',\n",
    "            'LinearRegression',\n",
    "            'Polynomial (3) SVC',\n",
    "            'Decision Tree',\n",
    "            'Random Forest',\n",
    "            'KNN-2',\n",
    "            'KNN-7',\n",
    "            'Logistic Regression',\n",
    "            'ZeroR',]\n",
    "    \n",
    "    # X_train, y_train, X_test = feature_filter(X_train, y_train, X_test)\n",
    "\n",
    "    for title, model in zip(titles[7:8], models[7:8]):\n",
    "        print(\"Running model\", title)\n",
    "        start = time.time()\n",
    "        \n",
    "        # X1, X2, y1, y2 = train_test_split(X_train, y_train, test_size=0.2)\n",
    "        # model.fit(X1, y1)\n",
    "        # y_pred = model.predict(X2)\n",
    "        # print(classification_report(y2, y_pred))\n",
    "        # print(\"Accuracy is\", accuracy_score(y2, y_pred))\n",
    "\n",
    "        acc = cross_validate(model, X_train, y_train)\n",
    "        print(\"Acc was\", np.mean(acc['test_score']), \"for model\", title)\n",
    "\n",
    "        #format_test_pred(model, X_train, y_train, X_test, title)\n",
    "\n",
    "        end = time.time()\n",
    "        t = end - start\n",
    "        print(\"Generated predictions for\", title)\n",
    "\n",
    "print(\"Running models on doc2vec set\")\n",
    "run_models(X1, y1, X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "def format_test_pred(model, X_train, y_train, X_test, title):\n",
    "        model.fit(X_train, y_train)\n",
    "        print(\"Fitted model\")\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(\"Made predictions\")\n",
    "        print(\"Max prediction\", max(y_pred))\n",
    "        y_pred = pd.Series(y_pred, name=\"rating_label\", index=range(1,len(y_pred)+1))\n",
    "        y_pred.index.name = \"id\"\n",
    "        y_pred.to_csv(f\"./model_predictions/{title}.csv\")\n",
    "\n",
    "def feature_filter(X_train, y_train, X_test):\n",
    "        selector = SelectKBest(k=20, score_func=mutual_info_classif)\n",
    "        print(\"Filtering features\")\n",
    "        X_train = pd.DataFrame(selector.fit_transform(X_train, y_train))\n",
    "        for feat_num in selector.get_support(indices=True):\n",
    "                print(selector.get_feature_names_out()[feat_num])\n",
    "\n",
    "        X_test = pd.DataFrame(selector.transform(X_test))\n",
    "        print(\"Chose best features as\", X_train.head())\n",
    "        return X_train, y_train, X_test\n",
    "\n",
    "\n",
    "def run_models(X_train, y_train, X_test):\n",
    "    model = LogisticRegression(max_iter = 1000)\n",
    "          \n",
    "    titles = ['Boosting']\n",
    "    \n",
    "    # X_train, y_train, X_test = feature_filter(X_train, y_train, X_test)\n",
    "    title = 'Boosting'\n",
    "        print(\"Running model\", title)\n",
    "        start = time.time()\n",
    "        \n",
    "        # X1, X2, y1, y2 = train_test_split(X_train, y_train, test_size=0.2)\n",
    "        # model.fit(X1, y1)\n",
    "        # y_pred = model.predict(X2)\n",
    "        # print(classification_report(y2, y_pred))\n",
    "        # print(\"Accuracy is\", accuracy_score(y2, y_pred))\n",
    "\n",
    "        format_test_pred(model, X_train, y_train, X_test, title)\n",
    "\n",
    "        end = time.time()\n",
    "        t = end - start\n",
    "        print(\"Generated predictions for\", title)\n",
    "\n",
    "print(\"Running models on doc2vec set\")\n",
    "run_models(X1, y1, X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
