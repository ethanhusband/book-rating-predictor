{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['desc-1', 'desc-60', 'desc-67', 'author-14', 'desc-64', 'desc-41',\n",
      "       'name-57', 'pagesNumber', 'PublishMonth', 'PublishYear', 'name-99',\n",
      "       'desc-87', 'desc-94', 'desc-55', 'name-79', 'desc-81', 'desc-78',\n",
      "       'desc-18', 'name-94', 'name-27', 'desc-62', 'name-16', 'desc-72',\n",
      "       'desc-77', 'desc-75', 'desc-33', 'desc-38', 'desc-29', 'name-98'],\n",
      "      dtype='object')\n",
      "(23063, 29)\n",
      "(23063,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import mutual_info_regression, mutual_info_classif\n",
    "\n",
    "def preprocess_training():\n",
    "    # Most of this is a task in preprocessing appropriately\n",
    "\n",
    "    train_data = pd.read_csv(\"./project_data_files/book_rating_train.csv\")\n",
    "\n",
    "    # These are strings we need to handle with provided files\n",
    "    train_name = pd.read_csv(\"./project_data_files/book_text_features_doc2vec/train_name_doc2vec100.csv\", index_col = False, delimiter = ',', header=None)\n",
    "    train_authors = pd.read_csv(\"./project_data_files/book_text_features_doc2vec/train_authors_doc2vec20.csv\", index_col = False, delimiter = ',', header=None)\n",
    "    train_desc = pd.read_csv(\"./project_data_files/book_text_features_doc2vec/train_desc_doc2vec100.csv\", index_col = False, delimiter = ',', header=None)\n",
    "\n",
    "\n",
    "    # Give these all column names which aren't ints\n",
    "    train_name = train_name.set_axis([f\"name-{x}\" for x in train_name.columns], axis=1)\n",
    "    train_authors = train_authors.set_axis([f\"author-{x}\" for x in train_authors.columns], axis=1)\n",
    "    train_desc = train_desc.set_axis([f\"desc-{x}\" for x in train_desc.columns], axis=1)\n",
    "\n",
    "    # Transform categorical values into useful vectors\n",
    "    train_lang = pd.get_dummies(train_data[['Language']])\n",
    "    train_lang = train_lang.drop(['Language_ara', 'Language_frs', 'Language_heb', 'Language_zho', 'Language_lat'], axis=1)\n",
    "\n",
    "    cv = CountVectorizer()\n",
    "    train_publisher = pd.DataFrame(cv.fit_transform(train_data[['Publisher']]).todense())\n",
    "    train_publisher = train_publisher.set_axis([f\"publisher-char-{x}\" for x in train_publisher.columns], axis=1)\n",
    "\n",
    "    # Merge all the pieces together\n",
    "    combine = pd.concat([train_name, train_desc, train_authors, train_publisher, train_lang], axis=1)\n",
    "\n",
    "    # Add the remaining attributes\n",
    "    X_train = pd.concat([combine, train_data[['PublishYear', 'PublishMonth', 'PublishDay', 'pagesNumber']]], axis=1).fillna(0)\n",
    "    y_train = train_data['rating_label']\n",
    "\n",
    "    return X_train, y_train\n",
    "\n",
    "# Get the preprocessed data for doc2vec\n",
    "X1, y1 = preprocess_training()\n",
    "\n",
    "def getFeatureCorr(X1, y1, excludeFours = False):\n",
    "    if (excludeFours):\n",
    "        combine = pd.concat([X1, y1], axis=1)\n",
    "        combine = combine[combine.rating_label != 4]\n",
    "        X1 = combine.drop(['rating_label'], axis=1)\n",
    "        y1 = combine[['rating_label']]\n",
    "\n",
    "    corr_matrix = X1.corrwith(y1)\n",
    "    best_features = corr_matrix.sort_values(ascending=False).head(15).index\n",
    "    return best_features\n",
    "\n",
    "def getFeatureMI(X1, y1, excludeFours = False):\n",
    "    if (excludeFours):\n",
    "        combine = pd.concat([X1, y1], axis=1)\n",
    "        combine = combine[combine.rating_label != 4]\n",
    "        X1 = combine.drop(['rating_label'], axis=1)\n",
    "        y1 = combine[['rating_label']]\n",
    "        \n",
    "\n",
    "    coeff_df =pd.DataFrame(mutual_info_classif(X1, y1).reshape(-1, 1), columns=['Coefficient'], index=X1.columns)\n",
    "    best_features = coeff_df.sort_values(by=['Coefficient'], ascending=False).head(15).index\n",
    "    return best_features\n",
    "\n",
    "# Feature selection\n",
    "X1 = X1[list(set(getFeatureCorr(X1, y1)).union(set(getFeatureMI(X1, y1))))]\n",
    "\n",
    "print(X1.columns)\n",
    "print(X1.shape)\n",
    "print(y1.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_testing():\n",
    "    # Most of this is a task in preprocessing appropriately\n",
    "\n",
    "    train_data = pd.read_csv(\"./project_data_files/book_rating_test.csv\")\n",
    "\n",
    "    # These are strings we need to handle with provided files\n",
    "    train_name = pd.read_csv(\"./project_data_files/book_text_features_doc2vec/test_name_doc2vec100.csv\", index_col = False, delimiter = ',', header=None)\n",
    "    train_authors = pd.read_csv(\"./project_data_files/book_text_features_doc2vec/test_authors_doc2vec20.csv\", index_col = False, delimiter = ',', header=None)\n",
    "    train_desc = pd.read_csv(\"./project_data_files/book_text_features_doc2vec/test_desc_doc2vec100.csv\", index_col = False, delimiter = ',', header=None)\n",
    "\n",
    "\n",
    "    # Give these all column names which aren't ints\n",
    "    train_name = train_name.set_axis([f\"name-{x}\" for x in train_name.columns], axis=1)\n",
    "    train_authors = train_authors.set_axis([f\"author-{x}\" for x in train_authors.columns], axis=1)\n",
    "    train_desc = train_desc.set_axis([f\"desc-{x}\" for x in train_desc.columns], axis=1)\n",
    "\n",
    "    # Transform categorical values into useful vectors\n",
    "    train_lang = pd.get_dummies(train_data[['Language']])\n",
    "    # Remove languages not found in the training data - they tell us nothing\n",
    "    train_lang = train_lang.drop(['Language_hun', 'Language_urd', 'Language_tha', 'Language_glg'], axis=1)\n",
    "\n",
    "    cv = CountVectorizer()\n",
    "    train_publisher = pd.DataFrame(cv.fit_transform(train_data[['Publisher']]).todense())\n",
    "    train_publisher = train_publisher.set_axis([f\"publisher-char-{x}\" for x in train_publisher.columns], axis=1)\n",
    "\n",
    "    # Merge all the pieces together\n",
    "    combine = pd.concat([train_desc, train_name, train_authors, train_publisher, train_lang], axis=1)\n",
    "\n",
    "    # Add the remaining attributes\n",
    "    X_train = pd.concat([combine, train_data[['PublishYear', 'PublishMonth', 'PublishDay', 'pagesNumber']]], axis=1).fillna(0)\n",
    "\n",
    "    return X_train\n",
    "\n",
    "X_test = preprocess_testing()\n",
    "X_test = X_test[list(set(getFeatureCorr(X1, y1)).union(set(getFeatureMI(X1, y1))))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running models on doc2vec set\n",
      "Running model Polynomial (3) SVC\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "def format_test_pred(model, X_train, y_train, X_test, title):\n",
    "        model.fit(X_train, y_train)\n",
    "        print(\"Fitted model\")\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(\"Made predictions\")\n",
    "        print(\"Max prediction\", max(y_pred))\n",
    "        y_pred = pd.Series(y_pred, name=\"rating_label\", index=range(1,len(y_pred)+1))\n",
    "        y_pred.index.name = \"id\"\n",
    "        y_pred.to_csv(f\"./model_predictions/{title}.csv\")\n",
    "\n",
    "def feature_filter(X_train, y_train, X_test):\n",
    "        selector = SelectKBest(k=20, score_func=mutual_info_classif)\n",
    "        print(\"Filtering features\")\n",
    "        X_train = pd.DataFrame(selector.fit_transform(X_train, y_train))\n",
    "        for feat_num in selector.get_support(indices=True):\n",
    "                print(selector.get_feature_names_out()[feat_num])\n",
    "\n",
    "        X_test = pd.DataFrame(selector.transform(X_test))\n",
    "        print(\"Chose best features as\", X_train.head())\n",
    "        return X_train, y_train, X_test\n",
    "\n",
    "\n",
    "def run_models(X_train, y_train, X_test):\n",
    "    models = [\n",
    "          AdaBoostClassifier(),\n",
    "          GaussianNB(),\n",
    "          MLPClassifier(),\n",
    "          LinearSVC(),\n",
    "          LinearRegression(),\n",
    "          SVC(kernel='poly', degree=3),\n",
    "          DecisionTreeClassifier(criterion=\"log_loss\"),\n",
    "          BaggingClassifier(base_estimator=DecisionTreeClassifier(splitter=\"random\"),n_estimators=10,\\\n",
    "                              max_samples=0.8, max_features=0.8),\n",
    "          KNeighborsClassifier(n_neighbors=2),\n",
    "          KNeighborsClassifier(n_neighbors=7),\n",
    "          LogisticRegression(max_iter = 5000),\n",
    "          DummyClassifier(strategy=\"most_frequent\"),\n",
    "          ]\n",
    "    titles = [\n",
    "        'ADA Boosting',\n",
    "        'GNB',\n",
    "            'MLP',\n",
    "            'LinearSVC',\n",
    "            'LinearRegression',\n",
    "            'Polynomial (3) SVC',\n",
    "            'Decision Tree',\n",
    "            'Random Forest',\n",
    "            'KNN-2',\n",
    "            'KNN-7',\n",
    "            'Logistic Regression',\n",
    "            'ZeroR',]\n",
    "    \n",
    "\n",
    "    for title, model in zip(titles, models):\n",
    "        print(\"Running model\", title)\n",
    "        start = time.time()\n",
    "        \n",
    "        # X1, X2, y1, y2 = train_test_split(X_train, y_train, test_size=0.2)\n",
    "        # model.fit(X1, y1)\n",
    "        # y_pred = model.predict(X2)\n",
    "        # print(classification_report(y2, y_pred))\n",
    "        # print(\"Accuracy is\", accuracy_score(y2, y_pred))\n",
    "        acc = cross_validate(model, X_train, y_train)\n",
    "        \n",
    "\n",
    "        #format_test_pred(model, X_train, y_train, X_test, title)\n",
    "\n",
    "        end = time.time()\n",
    "        t = end - start\n",
    "        print(\"Acc was\", np.mean(acc['test_score']), \"for model\", title, \"in time\", t)\n",
    "        print(\"Generated predictions for\", title)\n",
    "\n",
    "print(\"Running models on doc2vec set\")\n",
    "run_models(X1, y1, X_test)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
